import streamlit as st
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# =========================
# 1. Utils gÃ©ographiques
# =========================

def geocode_address(address: str):
    """Retourne (lat, lon) Ã  partir d'une adresse avec Nominatim (OpenStreetMap)."""
    url = "https://nominatim.openstreetmap.org/search"
    params = {
        "q": address,
        "format": "json",
        "limit": 1
    }
    resp = requests.get(url, params=params, headers={"User-Agent": "streamlit-footfall-app"})
    resp.raise_for_status()
    data = resp.json()
    if not data:
        return None, None
    return float(data[0]["lat"]), float(data[0]["lon"])


def fetch_pois_from_osm(lat: float, lon: float, radius_m: int = 500, max_pois: int = 10):
    """
    RÃ©cupÃ¨re des points d'intÃ©rÃªt significatifs autour d'un point via Overpass.
    On filtre sur quelques types de commerces et lieux trÃ¨s frÃ©quentÃ©s.
    """
    overpass_url = "https://overpass-api.de/api/interpreter"

    # Tags "importants" (commerce, transports, etc.)
    query = f"""
    [out:json][timeout:25];
    (
      node
        ["shop"~"supermarket|mall|department_store|convenience"]
        (around:{radius_m},{lat},{lon});
      node
        ["amenity"~"cinema|theatre|fast_food|restaurant|pub|bar|cafe|bank"]
        (around:{radius_m},{lat},{lon});
      node
        ["amenity"~"bus_station|ferry_terminal|marketplace"]
        (around:{radius_m},{lat},{lon});
      node
        ["railway"="station"]
        (around:{radius_m},{lat},{lon});
    );
    out body;
    >;
    out skel qt;
    """

    resp = requests.post(overpass_url, data=query, headers={"User-Agent": "streamlit-footfall-app"})
    resp.raise_for_status()
    data = resp.json()

    elements = data.get("elements", [])
    pois = []
    for el in elements:
        if el.get("type") != "node":
            continue
        tags = el.get("tags", {})
        name = tags.get("name")
        if not name:
            continue
        poi_type = tags.get("shop") or tags.get("amenity") or tags.get("railway") or "poi"
        pois.append({
            "id": el["id"],
            "name": name,
            "type": poi_type,
            "lat": el["lat"],
            "lon": el["lon"],
        })

    # On limite le nombre de POI pour rester raisonnable
    df_pois = pd.DataFrame(pois)
    if df_pois.empty:
        return df_pois

    # Petite prioritÃ© aux centres commerciaux / gares si prÃ©sents
    priority_keywords = ["mall", "station", "supermarket", "marketplace", "cinema"]
    df_pois["priority"] = df_pois["type"].apply(
        lambda t: 0 if any(pk in t for pk in priority_keywords) else 1
    )
    df_pois = df_pois.sort_values(["priority", "name"]).head(max_pois).reset_index(drop=True)
    return df_pois


# =========================================
# 2. Fournisseur de donnÃ©es de flux
# =========================================

def simulate_daily_footfall_for_poi(poi_id, start_date, end_date):
    """
    âš ï¸ FAUX fournisseur de donnÃ©es.
    Cette fonction gÃ©nÃ¨re une sÃ©rie temporelle jour par jour
    pour un POI donnÃ©, uniquement pour tester l'app.

    âœ Ã€ REMPLACER par :
      - un appel Ã  ton API Rennes
      - ou un fournisseur commercial (MyTraffic, telco, etc.)
    """
    rng = pd.date_range(start_date, end_date, freq="D")

    # Seed basÃ©e sur l'ID pour avoir un profil stable par POI
    np.random.seed(int(poi_id) % 2**32)

    # Base level de frÃ©quentation
    base = np.random.randint(300, 1500)

    # SaisonnalitÃ© hebdomadaire (moins de monde le dimanche par ex)
    weekday_effect = np.array([1.1, 1.05, 1.0, 1.0, 1.15, 1.4, 0.7])  # lunâ†’dim

    # Un peu de bruit
    noise = np.random.normal(0, base * 0.1, size=len(rng))

    values = []
    for i, d in enumerate(rng):
        factor = weekday_effect[d.weekday()]
        val = max(0, base * factor + noise[i])
        values.append(val)

    df = pd.DataFrame({"date": rng, "footfall": values})
    df["poi_id"] = poi_id
    return df


def get_daily_footfall_for_poi(poi_row, start_date, end_date):
    """
    Wrapper pour une source de donnÃ©es de flux.

    Pour passer en "vraie prod", il suffit de remplacer le contenu
    par des appels API rÃ©els, par ex :

      - fetch_from_rennes_api(poi_row['lat'], poi_row['lon'], start_date, end_date)
      - fetch_from_mytraffic(...)
      - fetch_from_google_popular_times(...)

    En gardant le mÃªme format de sortie (date, footfall, poi_id).
    """
    # Ici : on utilise le simulateur.
    return simulate_daily_footfall_for_poi(poi_row["id"], start_date, end_date)


# =========================
# 3. App Streamlit
# =========================

st.set_page_config(
    page_title="Analyse de flux - Multi-zones",
    layout="wide"
)

st.title("ğŸ“ˆ Analyse gÃ©nÃ©rale de flux de personnes par zone gÃ©ographique")
st.write(
    """
Appli **gÃ©nÃ©raliste** : tu dÃ©finis une zone (adresse ou coordonnÃ©es),  
on rÃ©cupÃ¨re les **points d'intÃ©rÃªt significatifs** (OSM) dans le rayon,  
puis on construit une **sÃ©rie quotidienne de flux** par POI et une **moyenne** sur la zone.
    
âš ï¸ Pour l'instant, les flux sont **simulÃ©s**.  
Il suffira de remplacer la fonction `get_daily_footfall_for_poi` par ta vraie API.
"""
)

# ---- ParamÃ¨tres de la zone ----
st.sidebar.header("ğŸ—ºï¸ ParamÃ¨tres de la zone")

mode = st.sidebar.radio(
    "Mode de saisie de la zone",
    ["Adresse", "Latitude / Longitude"],
    index=0
)

if mode == "Adresse":
    address = st.sidebar.text_input("Adresse / ville / lieu", "Rennes, France")
    lat = lon = None
else:
    lat = st.sidebar.number_input("Latitude", value=48.1173, format="%.6f")
    lon = st.sidebar.number_input("Longitude", value=-1.6778, format="%.6f")
    address = None

radius_m = st.sidebar.slider("Rayon de recherche (mÃ¨tres)", min_value=200, max_value=3000, value=800, step=100)

# PÃ©riode d'analyse
today = datetime.today().date()
default_start = today - timedelta(days=90)

start_date = st.sidebar.date_input("Date de dÃ©but", default_start)
end_date = st.sidebar.date_input("Date de fin", today)

if start_date > end_date:
    st.sidebar.error("La date de dÃ©but doit Ãªtre <= Ã  la date de fin.")

max_pois = st.sidebar.slider("Nombre maximum de POI Ã  analyser", 3, 30, 10)

run_button = st.sidebar.button("ğŸš€ Lancer l'analyse")

if run_button and start_date <= end_date:
    # 1) GÃ©ocodage
    with st.spinner("GÃ©ocodage de la zoneâ€¦"):
        if mode == "Adresse":
            lat, lon = geocode_address(address)
            if lat is None:
                st.error("Impossible de gÃ©ocoder cette adresse. Essaie d'Ãªtre plus prÃ©cis.")
                st.stop()
        # Sinon lat/lon dÃ©jÃ  fournis

    st.success(f"Zone centrÃ©e sur lat={lat:.5f}, lon={lon:.5f}")

    # 2) RÃ©cupÃ©ration des POI
    st.subheader("ğŸ“ Points d'intÃ©rÃªt identifiÃ©s")
    with st.spinner("Recherche des POI significatifs via OpenStreetMapâ€¦"):
        df_pois = fetch_pois_from_osm(lat, lon, radius_m=radius_m, max_pois=max_pois)

    if df_pois.empty:
        st.warning("Aucun point d'intÃ©rÃªt significatif trouvÃ© dans ce rayon. Essaie d'augmenter le rayon ou de changer de zone.")
        st.stop()

    st.dataframe(df_pois[["name", "type", "lat", "lon"]])

    # 3) RÃ©cupÃ©ration / simulation des sÃ©ries journaliÃ¨res
    st.subheader("ğŸ“Š SÃ©ries journaliÃ¨res par POI")

    all_series = []
    progress = st.progress(0)
    total = len(df_pois)

    for i, (_, poi) in enumerate(df_pois.iterrows(), start=1):
        df_ts = get_daily_footfall_for_poi(poi, start_date, end_date)
        df_ts["poi_name"] = poi["name"]
        df_ts["poi_type"] = poi["type"]
        all_series.append(df_ts)
        progress.progress(i / total)

    df_all = pd.concat(all_series, ignore_index=True)

    # 4) Visualisation dÃ©taillÃ©e
    tab1, tab2 = st.tabs(["DÃ©tail par POI", "Moyenne de la zone"])

    with tab1:
        st.markdown("### ğŸ“Œ DÃ©tail des flux par POI (simulÃ©s)")
        poi_selected = st.selectbox("Choisir un POI", df_pois["name"].tolist())
        df_one = df_all[df_all["poi_name"] == poi_selected].copy()
        df_one = df_one.sort_values("date")

        st.line_chart(
            df_one.set_index("date")["footfall"],
            height=300
        )
        st.write(df_one[["date", "footfall"]])

    # 5) AgrÃ©gation : moyenne de la zone
    with tab2:
        st.markdown("### ğŸ“Š Moyenne journaliÃ¨re de flux sur l'ensemble de la zone")

        df_zone = (
            df_all
            .groupby("date", as_index=False)["footfall"]
            .mean()
            .rename(columns={"footfall": "footfall_mean"})
        )

        st.line_chart(
            df_zone.set_index("date")["footfall_mean"],
            height=300
        )
        st.write(df_zone)

        # Export CSV
        csv = df_zone.to_csv(index=False).encode("utf-8")
        st.download_button(
            "ğŸ’¾ TÃ©lÃ©charger la moyenne journaliÃ¨re (CSV)",
            data=csv,
            file_name="footfall_zone_daily_mean.csv",
            mime="text/csv"
        )

    st.success("Analyse terminÃ©e (donnÃ©es simulÃ©es). Tu peux maintenant brancher ta vraie API de flux.")
